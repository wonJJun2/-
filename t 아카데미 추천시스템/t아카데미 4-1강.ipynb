{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"t아카데미 4-1강.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"yLqjxD2fgMhO"},"source":["# Base code : https://yamalab.tistory.com/92"]},{"cell_type":"code","metadata":{"trusted":true,"id":"9RGp1Ek4gMhP","executionInfo":{"status":"ok","timestamp":1627264197060,"user_tz":-540,"elapsed":394,"user":{"displayName":"jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXAoqqpvh36inEdauImW2OivAM6q7DWdNUfNDwZQ=s64","userId":"05969954628356184794"}}},"source":["import numpy as np\n","from tqdm import tqdm_notebook as tqdm\n","\n","import numpy as np\n","\n","# Base code : https://yamalab.tistory.com/92\n","class MatrixFactorization():\n","    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n","        \"\"\"\n","        :param R: rating matrix\n","        :param k: latent parameter\n","        :param learning_rate: alpha on weight update\n","        :param reg_param: beta on weight update\n","        :param epochs: training epochs\n","        :param verbose: print status\n","        \"\"\"\n","        self._R = R\n","        self._num_users, self._num_items = R.shape\n","        self._k = k\n","        self._learning_rate = learning_rate\n","        self._reg_param = reg_param\n","        self._epochs = epochs\n","        self._verbose = verbose\n","\n","\n","    def fit(self):\n","        \"\"\"\n","        training Matrix Factorization : Update matrix latent weight and bias\n","\n","        참고: self._b에 대한 설명\n","        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n","        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n","\n","        :return: training_process\n","        \"\"\"\n","\n","        # init latent features\n","        self._P = np.random.normal(size=(self._num_users, self._k))\n","        self._Q = np.random.normal(size=(self._num_items, self._k))\n","\n","        # init biases\n","        self._b_P = np.zeros(self._num_users)\n","        self._b_Q = np.zeros(self._num_items)\n","        self._b = np.mean(self._R[np.where(self._R != 0)])\n","\n","        # train while epochs\n","        self._training_process = []\n","        for epoch in range(self._epochs):\n","            # rating이 존재하는 index를 기준으로 training\n","            xi, yi = self._R.nonzero()\n","            for i, j in zip(xi, yi):\n","                self.gradient_descent(i, j, self._R[i, j])\n","            cost = self.cost()\n","            self._training_process.append((epoch, cost))\n","\n","            # print status\n","            if self._verbose == True and ((epoch + 1) % 10 == 0):\n","                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n","\n","\n","    def cost(self):\n","        \"\"\"\n","        compute root mean square error\n","        :return: rmse cost\n","        \"\"\"\n","\n","        # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n","        # 참고: http://codepractice.tistory.com/90\n","        xi, yi = self._R.nonzero()\n","        # predicted = self.get_complete_matrix()\n","        cost = 0\n","        for x, y in zip(xi, yi):\n","            cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n","        return np.sqrt(cost/len(xi))\n","\n","\n","    def gradient(self, error, i, j):\n","        \"\"\"\n","        gradient of latent feature for GD\n","\n","        :param error: rating - prediction error\n","        :param i: user index\n","        :param j: item index\n","        :return: gradient of latent feature tuple\n","        \"\"\"\n","\n","        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n","        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n","        return dp, dq\n","\n","\n","    def gradient_descent(self, i, j, rating):\n","        \"\"\"\n","        graident descent function\n","\n","        :param i: user index of matrix\n","        :param j: item index of matrix\n","        :param rating: rating of (i,j)\n","        \"\"\"\n","\n","        # get error\n","        prediction = self.get_prediction(i, j)\n","        error = rating - prediction\n","\n","        # update biases\n","        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n","        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n","\n","        # update latent feature\n","        dp, dq = self.gradient(error, i, j)\n","        self._P[i, :] += self._learning_rate * dp\n","        self._Q[j, :] += self._learning_rate * dq\n","\n","\n","    def get_prediction(self, i, j):\n","        \"\"\"\n","        get predicted rating: user_i, item_j\n","        :return: prediction of r_ij\n","        \"\"\"\n","        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n","\n","\n","    def get_complete_matrix(self):\n","        \"\"\"\n","        computer complete matrix PXQ + P.bias + Q.bias + global bias\n","\n","        - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n","        - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n","        - b를 더하는 것은 각 element마다 bias를 더해주는 것\n","\n","        - newaxis: 차원을 추가해줌. 1차원인 Latent들로 2차원의 R에 행/열 단위 연산을 해주기위해 차원을 추가하는 것.\n","\n","        :return: complete matrix R^\n","        \"\"\"\n","        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n","\n","\n","\n","# run example\n","if __name__ == \"__main__\":\n","    # rating matrix - User X Item : (7 X 5)\n","    R = np.array([\n","        [1, 0, 0, 1, 3],\n","        [2, 0, 3, 1, 1],\n","        [1, 2, 0, 5, 0],\n","        [1, 0, 0, 4, 4],\n","        [2, 1, 5, 4, 0],\n","        [5, 1, 5, 4, 0],\n","        [0, 0, 0, 1, 0],\n","    ])\n","\n","    # P, Q is (7 X k), (k X 5) matrix"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"AQpzxAFWgMhQ","outputId":"a465ffaf-6ba1-4623-9dce-ea7aa820dd34"},"source":["%%time\n","factorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\n","factorizer.fit()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration: 10 ; cost = 1.0362\n","Iteration: 20 ; cost = 0.8564\n","Iteration: 30 ; cost = 0.7445\n","Iteration: 40 ; cost = 0.6600\n","Iteration: 50 ; cost = 0.5946\n","Iteration: 60 ; cost = 0.5445\n","Iteration: 70 ; cost = 0.5062\n","Iteration: 80 ; cost = 0.4762\n","Iteration: 90 ; cost = 0.4519\n","Iteration: 100 ; cost = 0.4315\n","CPU times: user 156 ms, sys: 44.1 ms, total: 200 ms\n","Wall time: 147 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"9yQT1l0ngMhR","outputId":"f3657845-726b-4847-94cb-1e99164d01c7"},"source":["factorizer.get_complete_matrix()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.69231184, 4.63853214, 4.33123326, 1.04190297, 2.3019729 ],\n","       [1.10197004, 4.16824534, 2.89533771, 0.93480457, 1.92833881],\n","       [0.92760527, 1.99988635, 1.75570805, 4.92560566, 2.94174512],\n","       [1.74873035, 2.95751855, 2.44667857, 3.84030146, 3.42374881],\n","       [2.36349126, 1.07194376, 4.60608054, 4.03856125, 4.03716133],\n","       [4.65781574, 0.97895124, 5.24043736, 3.92064079, 6.50939479],\n","       [1.03946748, 0.95193674, 0.83997154, 1.20092167, 2.69881011]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"68s6f69cgMhS"},"source":["## 코드의 Step by Step 설명"]},{"cell_type":"markdown","metadata":{"id":"HFQBYGqRgMhS"},"source":["기본적인 구조는 위와 같습니다. 한번 __init__ 부터 차근차근 코드의 설명을 시작하겠습니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"zFDFRv2lgMhS"},"source":["class MatrixFactorization():\n","    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n","        \"\"\"\n","        :param R: rating matrix\n","        :param k: latent parameter\n","        :param learning_rate: alpha on weight update\n","        :param reg_param: beta on weight update\n","        :param epochs: training epochs\n","        :param verbose: print status\n","        \"\"\"\n","        self._R = R\n","        self._num_users, self._num_items = R.shape\n","        self._k = k\n","        self._learning_rate = learning_rate\n","        self._reg_param = reg_param\n","        self._epochs = epochs\n","        self._verbose = verbose"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HNN2TB2vgMhT"},"source":["일단, __init__ 함수는 Matrix Factorization이라는 클래스가 호출될 때 자동으로 실행되는 부분입니다. 파라미터로는 6개의 인자를 받는데 각각 아래의 의미를 가집니다.\n","\n","- R: 평점 행렬\n","- k: User Latent와 Item Latent의 차원의 수\n","- learning_rate: 학습률\n","- reg_param: Weight의 Regularization 값\n","- epochs: 전체 학습 횟수 (Total Epoch)\n","- verbose: 학습 과정을 출력할지 여부 (True : 10번마다 cost 출력, False : cost를 출력하지 않음)"]},{"cell_type":"code","metadata":{"trusted":true,"id":"UQNfxBQegMhT"},"source":["%%time\n","\n","R = np.array([\n","    [1, 0, 0, 1, 3],\n","    [2, 0, 3, 1, 1],\n","    [1, 2, 0, 5, 0],\n","    [1, 0, 0, 4, 4],\n","    [2, 1, 5, 4, 0],\n","    [5, 1, 5, 4, 0],\n","    [0, 0, 0, 1, 0],\n","])\n","\n","factorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\n","factorizer.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Szc18EuEgMhU"},"source":["위의 실행은 7개의 User, 5개의 Item에 대한 평점 행렬(R)에 대해서 k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True 와 같은 파라미터를 가지는 MatrixFactorization 객체를 생성하라는 의미입니다. 생성된 객체에서 factorizer.fit()을 통해서 fit() 함수를 실행하면, 아래의 코드가 실행되게 됩니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"JxuXvfu9gMhU"},"source":["def fit(self):\n","    \"\"\"\n","    training Matrix Factorization : Update matrix latent weight and bias\n","\n","    참고: self._b에 대한 설명\n","    - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n","    - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n","\n","    :return: training_process\n","    \"\"\"\n","\n","    # init latent features\n","    self._P = np.random.normal(size=(self._num_users, self._k))\n","    self._Q = np.random.normal(size=(self._num_items, self._k))\n","\n","    # init biases\n","    self._b_P = np.zeros(self._num_users)\n","    self._b_Q = np.zeros(self._num_items)\n","    self._b = np.mean(self._R[np.where(self._R != 0)])\n","\n","    # train while epochs\n","    self._training_process = []\n","    for epoch in range(self._epochs):\n","        # rating이 존재하는 index를 기준으로 training\n","        xi, yi = self._R.nonzero()\n","        for i, j in zip(xi, yi):\n","            self.gradient_descent(i, j, self._R[i, j])\n","        cost = self.cost()\n","        self._training_process.append((epoch, cost))\n","\n","        # print status\n","        if self._verbose == True and ((epoch + 1) % 10 == 0):\n","            print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_WrOEAIgMhU"},"source":["fit 함수에서 가장 먼저 실행되는 부분은 아래의 Latent Matrix를 초기화해주는 부분입니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"IIqDbsyXgMhU"},"source":["# init latent features\n","self._P = np.random.normal(size=(self._num_users, self._k))\n","self._Q = np.random.normal(size=(self._num_items, self._k))\n","\n","# init biases\n","self._b_P = np.zeros(self._num_users)\n","self._b_Q = np.zeros(self._num_items)\n","self._b = np.mean(self._R[np.where(self._R != 0)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qrhu_kegMhV"},"source":["이때, np.random.normal 함수를 이용해서 행렬을 초기화해주고 np.zeros 함수를 이용해서 bias 부분을 초기화해줍니다. 함수의 의미는 아래와 같습니다.\n","\n","- np.random.normal : (self._num_users, self._k)의 크기로 행렬을 정규분포 형태로 초기화합니다. 위의 예시에서는 User Latent Matrix는 (7, 3)의 크기를 Item Latent Matrix는 (5, 3)의 크기를 가짐\n","- np.zeros : self._num_users 혹은 self._num_items의 크기만큼의 0 값을 가지는 벡터를 생성합니다.\n","- np.mean(self._R[np.where(self._R != 0)]) : 전체 평점의 평균을 계산\n","\n","이후, 전체 학습 과정을 진행합니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"Ipnf7V0DgMhV"},"source":["# train while epochs\n","self._training_process = []\n","for epoch in range(self._epochs):\n","    # rating이 존재하는 index를 기준으로 training\n","    xi, yi = self._R.nonzero()\n","    for i, j in zip(xi, yi):\n","        self.gradient_descent(i, j, self._R[i, j])\n","    cost = self.cost()\n","    self._training_process.append((epoch, cost))\n","\n","    # print status\n","    if self._verbose == True and ((epoch + 1) % 10 == 0):\n","        print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gFOHXGsgMhV"},"source":["- self._training_process = [] 는 for문 안의 self._training_process.append((epoch, cost)) 에 사용되는 부분으로 학습 시에 Epoch와 Cost를 저장하는 부분입니다. for문의 경우 처음 파라미터로 받은 self._epochs 만큼 반복학습을 진행하게 됩니다.\n","\n","\n","- 먼저 시행되는 for문의 첫 번째로 시행되는 xi, yi = self._R.nonzero()는 평점 행렬에서 0이 아닌 부분 즉, 사용자가 평점을 매긴 부분에 대해서만 값을 추출하라는 의미입니다. 그 이유는 위의 이론에서 말했듯이 결측치가 아닌 부분을 통해서만 학습을 진행하려는 의도입니다. 이후에 해당 부분을 통해서 모든 평점 부분에 대해서 gradient_descent를 실행해줍니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"mv5P3NgggMhV"},"source":["for i, j in zip(xi, yi):\n","    self.gradient_descent(i, j, self._R[i, j])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ef14RnNHgMhV"},"source":["def gradient_descent(self, i, j, rating):\n","    \"\"\"\n","    graident descent function\n","\n","    :param i: user index of matrix\n","    :param j: item index of matrix\n","    :param rating: rating of (i,j)\n","    \"\"\"\n","\n","    # get error\n","    prediction = self.get_prediction(i, j)\n","    error = rating - prediction\n","\n","    # update biases\n","    self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n","    self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n","\n","    # update latent feature\n","    dp, dq = self.gradient(error, i, j)\n","    self._P[i, :] += self._learning_rate * dp\n","    self._Q[j, :] += self._learning_rate * dq"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9rw1l5VgMhV"},"source":["- gradient_descent 함수의 경우 행렬의 원소 위치(i, j)와 평점 값(self._R[i, j])을 받습니다. 바로 시작하는 prediction = self.get_prediction(i, j) 은 User Latent Matrix와 Item Latent Matrix의 곱을 통해서 평점 행렬의 값들을 생성하는 부분입니다.\n","\n","\n","- self._P[i, :].dot(self._Q[j, :].T) 에서 User Latent P와 Item Latent Q가 곱해져서 평점을 계산하고 Bias를 없애기 위해서 전체 평균(self._b), User의 평균 평점(self._b_P[i]), Item의 평균 평점(self._b_Q[j])을 더해줌으로써 값을 생성합니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"4tcPzrwKgMhV"},"source":["def get_prediction(self, i, j):\n","    \"\"\"\n","    get predicted rating: user_i, item_j\n","    :return: prediction of r_ij\n","    \"\"\"\n","    return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHUocgcqgMhW"},"source":["이후에, error = rating - prediction 을 통해서 얼마만큼의 차이가 있는지 계산을 합니다. 그리고 아래의 수식에서 사용한 공식을 이용해서 self.gradient(error, i, j) 에서 Gradient 값을 계산하고 Weight 및 Bais를 업데이트합니다."]},{"cell_type":"markdown","metadata":{"id":"BvPWkL3PgMhW"},"source":["![](https://drive.google.com/uc?export=view&id=1Nlhx0ilvzD4Vhxnj4WPrhnJNRFkRfBPf)"]},{"cell_type":"code","metadata":{"trusted":true,"id":"3psTZkGMgMhW"},"source":["# update biases\n","self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n","self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n","\n","# update latent feature\n","dp, dq = self.gradient(error, i, j)\n","self._P[i, :] += self._learning_rate * dp\n","self._Q[j, :] += self._learning_rate * dq"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LNNkgtAgMhW"},"source":["이후에, cost = self.cost() 에서 전체 Matrix에 대해서 오차를 계산하고 출력해줍니다. cost += pow(self._R[x, y] - self.get_prediction(x, y), 2) 에서 각 평점 별로 오차를 계산하게 됩니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"9NwfeHa3gMhW"},"source":["def cost(self):\n","    \"\"\"\n","    compute root mean square error\n","    :return: rmse cost\n","    \"\"\"\n","\n","    # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n","    # 참고: http://codepractice.tistory.com/90\n","    xi, yi = self._R.nonzero()\n","    # predicted = self.get_complete_matrix()\n","    cost = 0\n","    for x, y in zip(xi, yi):\n","        cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n","    return np.sqrt(cost/len(xi))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tz6-w5oIgMhW"},"source":["이후, 모든 Epoch에 대해서 학습이 완료되면 factorizer.get_complete_matrix() 를 통해서 완성된 평점 행렬을 추출하게 되면 SGD를 이용한 협업 필터링이 완료되게 됩니다."]},{"cell_type":"code","metadata":{"trusted":true,"id":"Dg7wIDJYgMhW"},"source":["factorizer.get_complete_matrix()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEG7FvG-gMhX"},"source":["참고로 이를 조금 수정해서 PyTorch를 이용한 코드는 다음의 링크에서 확인할 수 있습니다. 다음 포스팅에서는 ALS가 무엇인지에서부터 어떻게 사용하는지에 대해 알아보도록 하겠습니다."]}]}